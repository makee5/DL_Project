{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:47:49.941119Z",
     "start_time": "2024-12-24T12:47:49.938437Z"
    }
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"R_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"R_CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"R_USER_AGENT\")\n",
    "USERNAME = os.getenv(\"R_USERNAME\")\n",
    "PASSWORD = os.getenv(\"R_PASSWORD\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:47:50.428251Z",
     "start_time": "2024-12-24T12:47:50.424257Z"
    }
   },
   "id": "604b4f61620637a1",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reddit_instance = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:47:50.896810Z",
     "start_time": "2024-12-24T12:47:50.893505Z"
    }
   },
   "id": "27e15a06dbf4d104",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Subreddit(display_name='portugal2')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit = reddit_instance.subreddit('portugal2')\n",
    "subreddit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:47:51.415027Z",
     "start_time": "2024-12-24T12:47:51.413292Z"
    }
   },
   "id": "965a7713bba772",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "top_posts = subreddit.top(limit=1000, time_filter=\"year\")\n",
    "\n",
    "posts_df = []\n",
    "for post in top_posts:\n",
    "    # Converte o timestamp Unix para um formato de data legível\n",
    "    post_date = datetime.utcfromtimestamp(post.created_utc).strftime('%d-%m-%Y')\n",
    "\n",
    "    # Adiciona os dados do post à lista\n",
    "    posts_df.append({\n",
    "        'Title': post.title,\n",
    "        'Content': post.selftext,\n",
    "        'Number of Comments': post.num_comments,\n",
    "        'Created': post_date  # Data de criação\n",
    "    })\n",
    "posts_df = pd.DataFrame(posts_df)\n",
    "posts_df.to_excel('reddit.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:55:45.189676Z",
     "start_time": "2024-12-24T12:55:23.601159Z"
    }
   },
   "id": "e7130dccad701159",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Title  \\\n",
      "0  Namorado da Cristina Ferreira zangado.   \n",
      "1                  Vamos limpar Portugal!   \n",
      "2      Táxi falso no aeroporto de Lisboa    \n",
      "3                             É +/- isto    \n",
      "4                  Golden explanation...    \n",
      "\n",
      "                                             Content  Number of Comments  \\\n",
      "0                                                                     93   \n",
      "1  nata pessoas vão tornar possivel cristina rodr...                 610   \n",
      "2                        un turistas serem enganados                 137   \n",
      "3                                                                    132   \n",
      "4                                                                     67   \n",
      "\n",
      "      Created  \n",
      "0  21-05-2024  \n",
      "1  14-03-2024  \n",
      "2  10-07-2024  \n",
      "3  31-03-2024  \n",
      "4  26-04-2024  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Carregar as stopwords em português\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover caracteres especiais, mantendo apenas letras e números\n",
    "    text = re.sub(r'[^a-zá-úãõàâäçéêíóôúùñ\\s0-9]', '', text)\n",
    "    \n",
    "    # Remover números \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenizar as palavras\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lematizar as palavras\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_filtered]\n",
    "    \n",
    "    # Juntar as palavras lematizadas de volta em uma string\n",
    "    return ' '.join(words_lemmatized)\n",
    "\n",
    "# Aplicar o pré-processamento ao conteúdo\n",
    "posts_df['Content'] = posts_df['Content'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Salvar o DataFrame processado em um arquivo Excel\n",
    "posts_df.to_excel('reddit_cleaneds.xlsx', index=False)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame processado\n",
    "print(posts_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T13:04:58.297432Z",
     "start_time": "2024-12-24T13:04:58.063909Z"
    }
   },
   "id": "6a831c60e5b880a8",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "# Função para remover stopwords, números e deixar tudo em minúsculas\n",
    "def clean_text(text):\n",
    "    # Converter o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenizar o texto em palavras\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    words_filtered = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Juntar as palavras filtradas em uma string\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# Aplicar a função ao dataframe\n",
    "posts_df['Content'] = posts_df['Content'].apply(lambda x: clean_text(str(x)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c33046b71084c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a75370a8f388ff09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
