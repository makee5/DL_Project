{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Avaliação de sentimentos sobre filmes usando API do Reddit\n",
    "\n",
    "Neste projeto usamos técnicas de Tecnologia e Análise de Comportamentos para analisar reviews de filmes no subreddit \"TrueFilm\". \n",
    "\n",
    "As reviews estão classificadas como \"Negative\", \"Neutral\", \"Positive\" dando nos assim oportunidade de melhor análise dos sentimentos mais comuns sobre alguns filmes. \n",
    "\n",
    "Nesta subreddit existem muitas opiniões variadas sobre diversos filmes por isso decidimos retirar só os 250 comentários com mais interações do ultimo ano.   \n",
    "\n",
    "Usamos também a ferramenta PowerBI para realizar um dashboard que nos dá a oportunidade de ter uma melhor visão sobre o que acontece no nosso dataset, podendo assim tirar melhores conclusões e poder ver a informação de forma organizada.\n",
    "\n",
    "O objetivo deste trabalho é melhorar as nossas competencias na area de Tecnologia e Análise de Comportamentos e na comunicação dos resultados.\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afae30154d747549"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos a biblioteca praw, que nos dá acesso à API do Reddit "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a01de65fe76364"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:29.326805Z",
     "start_time": "2024-12-24T12:17:29.263548Z"
    }
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para termos acesso à API necessitamos de autenticar a nossa conta"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "438806fd03ea81a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"R_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"R_CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"R_USER_AGENT\")\n",
    "USERNAME = os.getenv(\"R_USERNAME\")\n",
    "PASSWORD = os.getenv(\"R_PASSWORD\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:30.558978Z",
     "start_time": "2024-12-24T12:17:30.552570Z"
    }
   },
   "id": "5d95db9f3d202d5d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MissingRequiredAttributeException",
     "evalue": "Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the Reddit class constructor, or as an environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMissingRequiredAttributeException\u001B[0m         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m reddit_instance \u001B[38;5;241m=\u001B[39m \u001B[43mpraw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReddit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCLIENT_ID\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_secret\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCLIENT_SECRET\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUSER_AGENT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43musername\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUSERNAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPASSWORD\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ML_ADS/lib/python3.10/site-packages/praw/util/deprecate_args.py:46\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[1;32m     40\u001B[0m     warn(\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m     44\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m     45\u001B[0m     )\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m_old_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ML_ADS/lib/python3.10/site-packages/praw/reddit.py:259\u001B[0m, in \u001B[0;36mReddit.__init__\u001B[0;34m(self, site_name, config_interpolation, requestor_class, requestor_kwargs, token_manager, **config_settings)\u001B[0m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attribute \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser_agent\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig, attribute) \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mCONFIG_NOT_SET, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 259\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MissingRequiredAttributeException(\n\u001B[1;32m    260\u001B[0m             required_message\u001B[38;5;241m.\u001B[39mformat(attribute)\n\u001B[1;32m    261\u001B[0m         )\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mclient_secret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mCONFIG_NOT_SET:\n\u001B[1;32m    263\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequired_message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclient_secret\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFor installed applications this value must be set to None via a keyword argument to the Reddit class constructor.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mMissingRequiredAttributeException\u001B[0m: Required configuration setting 'client_id' missing. \nThis setting can be provided in a praw.ini file, as a keyword argument to the Reddit class constructor, or as an environment variable."
     ]
    }
   ],
   "source": [
    "reddit_instance = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:31.296362Z",
     "start_time": "2024-12-24T12:17:31.109498Z"
    }
   },
   "id": "ed9a1ba621b68976",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usamos a função *subreddit('TrueFilm')* para entrarmos na subreddit que nos interessa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22db930b5139c914"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reddit_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m subreddit \u001B[38;5;241m=\u001B[39m \u001B[43mreddit_instance\u001B[49m\u001B[38;5;241m.\u001B[39msubreddit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrueFilm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m subreddit\n",
      "\u001B[0;31mNameError\u001B[0m: name 'reddit_instance' is not defined"
     ]
    }
   ],
   "source": [
    "subreddit = reddit_instance.subreddit('TrueFilm')\n",
    "subreddit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:33.620707Z",
     "start_time": "2024-12-24T12:17:33.610402Z"
    }
   },
   "id": "634f0de29f77ac87",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Com os seguintes codigos podemos observar algumas informações sobre o subreddit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea075711bca36d7b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43msubreddit\u001B[49m\u001B[38;5;241m.\u001B[39mtitle)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'subreddit' is not defined"
     ]
    }
   ],
   "source": [
    "print(subreddit.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:36.281235Z",
     "start_time": "2024-12-24T12:17:36.272858Z"
    }
   },
   "id": "b485b8a6f4dd1ed3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43msubreddit\u001B[49m\u001B[38;5;241m.\u001B[39mdescription)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'subreddit' is not defined"
     ]
    }
   ],
   "source": [
    "print(subreddit.description)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:17:36.624909Z",
     "start_time": "2024-12-24T12:17:36.621395Z"
    }
   },
   "id": "3ed65c5b081b4b0e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(subreddit.subscribers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6401767b34e2f059",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para acessar os comentários com mais interações, utilizamos a função *top* com os parâmetros *limit* e *time_filter*, que respetivamente restringem o número de posts e definem o período de tempo considerado.\n",
    "\n",
    "Criamos também um ciclo for para acessar as todos os 250 comentários retirando o titulo, conteudo e data de publicação e alteramos a data para o formato YYYY-MM-DD HH:MM:SS e guardamos tudo num Excel chamado *reddit_top_posts*."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5e45f68d408cc9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "top_posts = subreddit.top(limit=250, time_filter=\"year\")\n",
    "\n",
    "posts_data = []\n",
    "for post in top_posts:\n",
    "    # Converte o timestamp Unix para um formato de data legível\n",
    "    post_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Adiciona os dados do post à lista\n",
    "    posts_data.append({\n",
    "        'Title': post.title,\n",
    "        'Content': post.selftext,\n",
    "        'Created': post_date  # Data de criação\n",
    "    })\n",
    "df = pd.DataFrame(posts_data)\n",
    "df.to_excel('reddit_top_posts.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb7d093a58ca76aa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pré-Processamento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ea27bdaf0da6e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primeiro, para conseguirmos extrair mais informação retiramos as stopwords dos comentários.\n",
    "\n",
    "Depois utilizamos o código para analisar o sentimento dos comentários retirados do Excel *reddit_top_posts*. Para isso, carregamos um modelo de classificação de sentimento pré-treinado (RoBERTa) e seu tokenizer, ambos disponíveis através da biblioteca `transformers`.\n",
    "\n",
    "Foi criada uma função chamada *analyze_sentiment* que recebe o conteúdo do post, tokeniza o texto e o passa pelo modelo para gerar uma previsão de sentimento. O modelo retorna as probabilidades de cada classe (negativo, neutro, positivo) e o sentimento mais provável é selecionado. \n",
    "\n",
    "Em seguida, aplicamos essa função ao conteúdo dos posts, criando uma nova coluna no DataFrame chamada *sentiment*. Por fim, o DataFrame atualizado, contendo as previsões de sentimento, é salvo num novo arquivo Excel chamado *reddit_top_posts_with_sentiment*."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0e86940e393e898"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Carrega o dataset dos posts que criaste\n",
    "df = pd.read_excel('reddit_top_posts.xlsx')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Função para remover stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)  # Tokenizar o texto em palavras\n",
    "    words_filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# Aplicar a função na coluna de textos\n",
    "df['Content'] = df['Content'].apply(remove_stopwords)\n",
    "\n",
    "# Carrega o modelo e o tokenizer RoBERTa\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "# Função para analisar o sentimento do texto com tokenização manual\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) and pd.notnull(text):\n",
    "        # Tokeniza o texto\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # Passa pelo modelo\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Obtém as probabilidades e determina o rótulo de sentimento\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        sentiment_label = torch.argmax(probs).item()\n",
    "        \n",
    "        # Mapeia os índices para os rótulos de sentimento: 0=Negative, 1=Neutral, 2=Positive\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        return labels[sentiment_label]\n",
    "    else:\n",
    "        return \"No analysis\"\n",
    "\n",
    "# Aplica a função de análise de sentimento ao conteúdo do post e cria uma nova coluna 'sentiment'\n",
    "df['sentiment'] = df['Content'].apply(analyze_sentiment)\n",
    "\n",
    "# Salva o DataFrame com a nova coluna de sentimento em um novo arquivo Excel\n",
    "df.to_excel('reddit_top_posts_with_sentiment.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeceb0ac757feabe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dashboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8def87c3fac7612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"power_bi.png\" height=\"1000\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87d2715a649a9a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusão"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e89530cb887dd4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A ferramenta utilizada para o desenvolvimento do dashboard foi o Power BI, escolhida por já termos experiência prévia com ela em outra cadeira, o que facilitou o processo. O objetivo principal foi realizar uma Análise de Sentimentos das Reviews de Filmes. Criamos um gráfico de barras que apresenta a quantidade de comentários classificados como neutros, positivos e negativos. No entanto, como os números exatos não eram facilmente visíveis no gráfico, adicionamos uma \"caixa de cartões\", que exibe o total de comentários.\n",
    "Para aprofundar a análise, incluímos três filtros interativos: o primeiro, de ano e mês, para identificar os períodos com maior volume de comentários, o segundo, de dias da semana, para descobrir em quais dias as pessoas mais comentam ou interagem e o terceiro, de título e texto do comentário, para explorar detalhes específicos e identificar rapidamente sentimentos positivos ou negativos. Além disso, criamos um segundo gráfico de barras, que mostra os dias e meses com maior volume de comentários, permitindo uma visão mais clara das tendências temporais.\n",
    "Para complementar o dashboard, adicionamos uma \"Word Cloud\" (Nuvem de Palavras), que destacou as palavras mais frequentes nos comentários, como nomes de filmes ou adjetivos comuns. Também implementamos uma barra de pesquisa, que possibilitou a busca de qualquer palavra, frase ou nome de filme, mostrando se os comentários que contêm essa palavra foram classificados como positivos, neutros ou negativos, bem como os dias e anos em que apareceram e outras palavras associadas.\n",
    "Com base no dataset analisado, concluímos que o mês com maior volume de comentários foi março, o dia da semana mais comentado foi a quarta-feira, e a maioria dos comentários apresenta um sentimento neutro. Em resumo, o dashboard possibilitou uma análise clara e interativa, identificando tendências de comportamento nos comentários e oferecendo insights úteis sobre a receção de filmes em diferentes períodos e contextos. Essas conclusões reforçam o potencial do Power BI para transformar dados complexos em informações acessíveis e significativas."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fbaec148f8bc85c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3500d6a5a0c7afaf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d20c0d65b1e2c0c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
