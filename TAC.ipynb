{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Avaliação de sentimentos sobre filmes usando API do Reddit\n",
    "\n",
    "Neste projeto usamos técnicas de Tecnologia e Análise de Comportamentos para analisar reviews de filmes no subreddit \"TrueFilm\". \n",
    "\n",
    "As reviews estão classificadas como \"Negative\", \"Neutral\", \"Positive\" dando nos assim oportunidade de melhor análise dos sentimentos mais comuns sobre alguns filmes. \n",
    "\n",
    "Nesta subreddit existem muitas opiniões variadas sobre diversos filmes por isso decidimos retirar só os 250 comentários com mais interações do ultimo ano.   \n",
    "\n",
    "Usamos também a ferramenta PowerBI para realizar um dashboard que nos dá a oportunidade de ter uma melhor visão sobre o que acontece no nosso dataset, podendo assim tirar melhores conclusões e poder ver a informação de forma organizada.\n",
    "\n",
    "\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afae30154d747549"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui importamos a biblioteca praw, que nos dá acesso à API do Reddit "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a01de65fe76364"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T15:13:21.853526Z",
     "start_time": "2024-12-13T15:13:21.746113Z"
    }
   },
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para termos acesso à API necessitamos de autenticar a nossa conta"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "438806fd03ea81a4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"R_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"R_CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"R_USER_AGENT\")\n",
    "USERNAME = os.getenv(\"R_USERNAME\")\n",
    "PASSWORD = os.getenv(\"R_PASSWORD\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:47.533537Z",
     "start_time": "2024-12-12T15:28:47.525843Z"
    }
   },
   "id": "5d95db9f3d202d5d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reddit_instance = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:51.329932Z",
     "start_time": "2024-12-12T15:28:51.161040Z"
    }
   },
   "id": "ed9a1ba621b68976",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usamos o codigo .subreddit('TrueFilm') para entrarmos na subreddit que nos interessa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22db930b5139c914"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Subreddit(display_name='TrueFilm')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit = reddit_instance.subreddit('TrueFilm')\n",
    "subreddit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:51.894293Z",
     "start_time": "2024-12-12T15:28:51.890369Z"
    }
   },
   "id": "634f0de29f77ac87",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Com os seguintes codigos podemos observar algumas informações sobre o subreddit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea075711bca36d7b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueFilm: An in-depth discussion of film\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:53.046349Z",
     "start_time": "2024-12-12T15:28:53.041826Z"
    }
   },
   "id": "b485b8a6f4dd1ed3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###About Us\n",
      "-\n",
      "\n",
      "/r/TrueFilm is a subreddit for in-depth discussions about film.\n",
      "\n",
      "We want to encourage and support in-depth, intellectual discussion. Clear, polite and well-written responses should be upvoted; opinions should not be downvoted.\n",
      "\n",
      "---\n",
      "\n",
      "###Rules [(Expanded)](https://www.reddit.com/r/TrueFilm/about/rules/)\n",
      "\n",
      "**General:**\n",
      "\n",
      "1. All discussion must be related to film.\n",
      "\n",
      "2. No racism, sexism, or other forms of bigotry.\n",
      "\n",
      "3. Moderators have final discretion.\n",
      "\n",
      "**Posts:**\n",
      "\n",
      "4. Threads must promote in-depth discussion.\n",
      "\n",
      "5. Threads must point discussion in a specific direction.\n",
      "\n",
      "6. Links to outside articles must be submitted in a self-post and are subject to the above posting rules. [(Click for video essays)](https://redd.it/64fj4m)\n",
      "\n",
      "**Comments:**\n",
      "\n",
      "7. Be civil and don’t downvote opinions.\n",
      "\n",
      "8. There is a 180 character minimum for top-level comments.\n",
      "\n",
      "---\n",
      "\n",
      "###Follow us on:\n",
      "\n",
      "- [**LETTERBOXD**](http://letterboxd.com/truefilmreddit/) \n",
      "\n",
      "- [**TWITTER**](https://twitter.com/truefilmreddit)\n",
      "\n",
      "---\n",
      "\n",
      "###TrueFilm Resources:\n",
      "\n",
      "- [**TRUEFILM ARCHIVES**](https://goo.gl/2n2miP)\n",
      "\n",
      "---\n",
      "\n",
      "###TrueFilm Projects\n",
      "\n",
      "[Fun and Fancy Free Discussion](/r/TrueFilm/search?q=flair%3A%27FFF%27&sort=new&restrict_sr=on&t=all)\n",
      "\n",
      "[Theme Months](/r/TrueFilm/search?q=flair%3A%27TM%27&sort=new&restrict_sr=on&t=all)\n",
      "\n",
      "[What Have You Been Watching](/r/TrueFilm/search?q=flair%3A%27WHYBW%27&restrict_sr=on&sort=new&t=all)\n",
      "\n",
      "[Better Know a Director](/r/TrueFilm/search?q=flair%3A%27BKD%27&restrict_sr=on&sort=new&t=all)\n",
      "\n",
      "[TrueFilm Netflix Club](/r/TrueFilm/search?q=flair%3A%27TFNC%27&restrict_sr=on&sort=new&t=all)\n",
      "\n",
      "---\n",
      "\n",
      "[](https://reddit.com/r/truefilm)\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.description)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:53.079439Z",
     "start_time": "2024-12-12T15:28:53.065836Z"
    }
   },
   "id": "3ed65c5b081b4b0e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484077\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.subscribers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:28:53.768597Z",
     "start_time": "2024-12-12T15:28:53.765413Z"
    }
   },
   "id": "6401767b34e2f059",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para acessar os comentários com mais interações, utilizamos a função *top* com os parâmetros *limit* e *time_filter*, que respectivamente restringem o número de posts e definem o período de tempo considerado.\n",
    "\n",
    "Criamos também um ciclo for para acessar as todos os 250 comentários retirando o titulo, conteudo e data de publicação e alteramos a data para o formato YYYY-MM-DD HH:MM:SS e guardamos tudo num Excel chamado *reddit_top_posts*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5e45f68d408cc9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "top_posts = subreddit.top(limit=250, time_filter=\"year\")\n",
    "\n",
    "posts_data = []\n",
    "for post in top_posts:\n",
    "    # Converte o timestamp Unix para um formato de data legível\n",
    "    post_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Adiciona os dados do post à lista\n",
    "    posts_data.append({\n",
    "        'Title': post.title,\n",
    "        'Content': post.selftext,\n",
    "        'Created': post_date  # Data de criação\n",
    "    })\n",
    "df = pd.DataFrame(posts_data)\n",
    "df.to_excel('reddit_top_posts.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:29:00.472433Z",
     "start_time": "2024-12-12T15:28:55.431683Z"
    }
   },
   "id": "fb7d093a58ca76aa",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizamos o código para analisar o sentimento dos comentários retirados do Excel *reddit_top_posts*. Para isso, carregamos um modelo de classificação de sentimento pré-treinado (RoBERTa) e seu tokenizer, ambos disponíveis através da biblioteca `transformers`.\n",
    "\n",
    "Foi criada uma função chamada *analyze_sentiment* que recebe o conteúdo do post, tokeniza o texto e o passa pelo modelo para gerar uma previsão de sentimento. O modelo retorna as probabilidades de cada classe (negativo, neutro, positivo) e o sentimento mais provável é selecionado. \n",
    "\n",
    "Em seguida, aplicamos essa função ao conteúdo dos posts, criando uma nova coluna no DataFrame chamada *sentiment*. Por fim, o DataFrame atualizado, contendo as previsões de sentimento, é salvo num novo arquivo Excel chamado *reddit_top_posts_with_sentiment*."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0e86940e393e898"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Carrega o dataset dos posts que criaste\n",
    "df = pd.read_excel('reddit_top_posts.xlsx')\n",
    "\n",
    "# Carrega o modelo e o tokenizer RoBERTa\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "# Função para analisar o sentimento do texto com tokenização manual\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) and pd.notnull(text):\n",
    "        # Tokeniza o texto\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # Passa pelo modelo\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Obtém as probabilidades e determina o rótulo de sentimento\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        sentiment_label = torch.argmax(probs).item()\n",
    "        \n",
    "        # Mapeia os índices para os rótulos de sentimento: 0=Negative, 1=Neutral, 2=Positive\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        return labels[sentiment_label]\n",
    "    else:\n",
    "        return \"No analysis\"\n",
    "\n",
    "# Aplica a função de análise de sentimento ao conteúdo do post e cria uma nova coluna 'sentiment'\n",
    "df['sentiment'] = df['Content'].apply(analyze_sentiment)\n",
    "\n",
    "# Salva o DataFrame com a nova coluna de sentimento em um novo arquivo Excel\n",
    "df.to_excel('reddit_top_posts_with_sentiment.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T15:29:46.665694Z",
     "start_time": "2024-12-12T15:29:03.351850Z"
    }
   },
   "id": "aeceb0ac757feabe",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dashboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8def87c3fac7612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"power_bi.png\" height=\"1000\"> "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87d2715a649a9a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Os alunos deverão desenvolver uma dashboard para visualizar os resultados da análise de\n",
    "sentimentos. Esta dashboard pode ser criada utilizando ferramentas como Power BI,\n",
    "Tableau, ou Dash (Python). A visualização deve incluir gráficos e métricas que ajudem a\n",
    "interpretar os dados, como a distribuição de sentimentos, tendências ao longo do tempo e\n",
    "comparações entre diferentes categorias (por exemplo, produtos, filmes, etc.)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "783e3fff6b13d5df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A ferramenta que utilizamos para o dasboard foi o Power BI, pois como já usamos em outra cadeira esta ferramenta. Primeiramente "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f885811cdcdec768"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
